/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * License); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * AS IS BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

/*
 * Author: Wo-Ki
 */

#include "test_op.h"
#include "../operator/prototype/convolution_param.h"

static int float_mismatch(float* current, float* reference, int size)
{
    for (int i = 0; i < size; i++)
    {
        float tmp = fabs(current[i]) - fabs(reference[i]);
        fprintf(stderr, "index:%d, a:%f, b:%f\n", i, current[i], reference[i]);
        if (fabs(tmp) > 0.5)
        {
            fprintf(stderr, "test failed, index:%d, a:%f, b:%f\n\n", i, current[i], reference[i]);
            //            return -1;
        }
    }
    fprintf(stderr, "test pass\n");

    return 0;
}

int create_test_conv_node(graph_t graph, const char* input_name, const char* node_name, int data_type, int layout, int n, int c, int h, int w)
{
    (void)layout;
    (void)n;
    (void)c;
    (void)h;
    (void)w;

    /* create the test node */
    struct node* test_node = (struct node*)create_graph_node(graph, node_name, "Convolution");

    tensor_t input_tensor = get_graph_tensor(graph, input_name);

    if (NULL == input_tensor)
    {
        fprintf(stderr, "create test node failed.\n");
        return -1;
    }

    /* create the sub node to product another input tensors which the test node is needed, such as weight/bias/slope tensor. */
    /* weight */
    node_t weight_node = create_graph_node(graph, "weight", "Const");
    tensor_t weight_tensor = create_graph_tensor(graph, "weight", TENGINE_DT_INT8);
    set_node_output_tensor(weight_node, 0, weight_tensor, TENSOR_TYPE_CONST);
    int weight_dims[4] = {6, 3, 5, 5}; // channel num
    set_tensor_shape(weight_tensor, weight_dims, 4);

    /* bias */
    node_t bias_node = create_graph_node(graph, "bias", "Const");
    tensor_t bias_tensor = create_graph_tensor(graph, "bias", TENGINE_DT_INT32);
    set_node_output_tensor(bias_node, 0, bias_tensor, TENSOR_TYPE_CONST);
    int bias_dims[1] = {6}; // channel num
    set_tensor_shape(bias_tensor, bias_dims, 1);

    /* input tensors of test node */
    set_node_input_tensor(test_node, 0, input_tensor);
    set_node_input_tensor(test_node, 1, weight_tensor);
    set_node_input_tensor(test_node, 2, bias_tensor);

    /* output tensors of test node */
    tensor_t output_tensor = create_graph_tensor(graph, node_name, data_type);
    set_node_output_tensor(test_node, 0, output_tensor, TENSOR_TYPE_VAR);

    /* set params */
    struct conv_param* conv_param = (struct conv_param*)(struct node*)test_node->op.param_mem;

    conv_param->kernel_h = 5;
    conv_param->kernel_w = 5;
    conv_param->stride_h = 1;
    conv_param->stride_w = 1;
    conv_param->pad_h0 = 0;
    conv_param->pad_h1 = 0;
    conv_param->pad_w0 = 0;
    conv_param->pad_w1 = 0;
    conv_param->dilation_h = 1;
    conv_param->dilation_w = 1;
    conv_param->input_channel = 3;
    conv_param->output_channel = 7;
    conv_param->group = 1;
    conv_param->activation = -1;

    return 0;
}

/* quant params */
/*
 * scale = (max(abs(min), abs(max))) / 127
 * zero_point = 0
 * float32 = int8 * scale
 */


int test_hcw_op_conv_main()
{
    printf("test_hcw_op_conv_main...\n");
    float input_scale = 0.3937f;
    float weight_scales[2] = {0.23622f, 0.07874f};
    float bias_scales[2] = {0.023622f * input_scale, 0.007874f * input_scale};
    float output_scale = 0.020078f;

    int input_zero_point = 0;
    int weight_zero_point[2] = {0, 0};
    int bias_zero_point[2] = {0, 0};
    int output_zero_point = 0;

    /* float32 data */
    //    int8_t reference_out[2][4] = {{6, 1,
    //                                  15, 27},
    //                                 {-4, 2,
    //                                  4, 10}};
    //    float reference_out[8] = {6, 1,
    //                              15, 27,
    //                              -3, 3,
    //                              5, 11};
    float reference_out[8] = {6, 1,
                              15, 27,
                              1, 7,
                              9, 15};
    //    int8_t input_data[32] = {1, -2, 1, 3,
    //                             -2, 3, 3, 5,
    //                             1, 0, -5, 5,
    //                             0, 0, 0, -1,
    //                             1, -2, 1, 3,
    //							  -2, 3, 3, 5,
    //							  1, 0, -5, 5,
    //							  0, 0, 0, -1};
    //    int8_t input_data[75] ={
    //    			-41,-39,-37,-53,-60,
    //        		-38,-38,-41,-57,-49,
    //        		-46,-43,-49,-61,-61,
    //        		-56,-51,-56,-51,-25,
    //        		-60,-64,-65,-50,-18,
    //
    //        		-48,-33,-27,-39,-37,
    //        		-43,-33,-35,-34,-20,
    //        		-47,-41,-27,-37,-40,
    //        		-57,-57,-12,-10,3,
    //        		-56,-59,-36,-19,22,
    //
    //        		-76,-75,-74,-69,-67,
    //        		-74,-78,-74,-63,-47,
    //        		-74,-77,-59,-65,-65,
    //        		-75,-78,-46,-43,-24,
    //        		-74,-78,-67,-47,-8,
    //
    //    };
    int8_t input_data[3072] = {
        -41,-39,-37,-53,-60,-59,-73,-66,-35,-68,-63,-69,-76,-68,-67,-54,-58,-56,-64,-42,13,-53,-64,-65,-57,-40,-58,-64,-21,-56,-55,-53,
        -38,-38,-41,-57,-49,-60,-37,-56,-56,-34,-81,-62,-29,-68,-62,-52,-54,-59,-68,-46,-56,-55,-68,-66,-49,-38,-63,-63,-10,-49,-64,-39,
        -46,-43,-49,-61,-61,-59,-54,2,-68,-72,-52,-57,3,80,-56,-55,-51,-57,-61,-60,-60,42,-37,-44,-45,-47,-52,-48,-35,-55,-57,-8,
        -56,-51,-56,-51,-25,-60,-69,-38,-49,-64,-68,-41,11,14,-45,-55,-8,-45,-54,-61,14,-2,-40,-58,-24,-44,-23,-26,-59,-17,-13,-13,
        -60,-64,-65,-50,-18,-55,-51,-56,-68,-71,-70,4,-6,14,-4,-64,-46,-14,-3,-42,19,-2,-68,-67,-40,-31,-2,-21,-51,-60,-37,-45,
        -66,-63,-64,-59,-35,-38,-65,-67,-68,-73,-83,-68,-8,7,34,-26,-45,-43,-68,19,39,16,-63,-56,-39,-14,2,-2,-42,-53,-34,-47,
        -64,-62,-63,-63,-41,-41,-55,-65,-67,-76,-73,1,-17,-10,11,11,19,23,34,26,33,40,-29,-60,-54,-57,-61,5,-52,-46,-37,-43,
        -64,-66,-64,-64,-72,-58,-60,-37,-71,-74,-68,46,-30,-8,-14,7,21,-4,-3,19,35,86,-60,-66,-57,-56,-52,-41,-49,-53,-47,-37,
        -65,-64,-64,-64,-72,-65,-51,-26,-73,-72,-63,-8,-9,-24,15,13,8,17,-4,15,21,-18,-68,-49,-61,-56,-53,-64,-48,-42,-34,-29,
        -64,-67,-59,-64,-71,-72,-52,-30,-71,-70,-70,20,-2,10,-25,13,32,37,-2,18,2,-46,-67,-60,-60,-55,-50,-45,-32,-35,-45,-33,
        -64,-64,-56,-54,-59,-68,-53,1,-42,-56,-52,-19,-29,-5,100,29,77,6,26,10,27,-39,-60,-48,-56,-60,-58,-60,-24,-60,-57,4,
        -20,-67,-60,-64,-19,-62,-70,-9,-39,-47,-22,-20,-9,13,31,28,48,-59,56,3,59,-18,-29,-25,-8,-38,-68,-32,-19,-52,-46,-23,
        -54,-41,-35,-60,-33,-22,-64,-48,-51,-49,-49,-8,-45,-50,15,44,-10,41,-16,-6,52,14,15,-49,-10,-30,-49,-41,-16,-47,-25,-47,
        -29,-44,-43,-37,-37,-48,-41,-12,-44,-45,-52,2,-14,8,38,-41,52,41,-11,10,50,17,-15,-2,-52,-58,-49,-47,-32,-18,-52,-32,
        -36,-11,-56,-43,-52,-55,-59,-60,-33,-52,-50,-37,-1,31,12,26,52,90,25,65,19,29,2,-29,-65,-27,-19,-31,-42,-49,-59,-45,
        -29,-7,-37,-32,-70,-37,-56,-18,-49,-57,-37,-11,10,21,9,26,91,104,85,64,10,28,22,-13,-59,-66,-55,-47,-49,-37,-57,-54,
        -14,-28,-45,-56,-49,-55,-35,-49,25,-60,-43,-6,0,8,32,34,66,83,53,37,10,30,33,-41,-40,-46,-56,-48,-67,-35,-44,-33,
        -62,-51,-41,-40,-30,-61,-54,-60,-1,-60,-45,-10,16,17,59,71,87,26,40,33,22,34,40,-54,80,-56,-59,-52,-53,-60,-52,-60,
        -26,-64,-50,-9,-3,-49,-45,-44,-38,-17,-14,-6,7,55,68,68,54,28,18,31,28,27,52,-11,-56,11,-54,-21,-50,7,-56,-61,
        -33,-45,-41,-37,-28,17,-25,-41,-64,-49,-10,17,-6,21,64,61,41,23,33,37,2,54,68,-3,-58,-32,-22,-39,-37,-34,-40,-51,
        -8,-38,-56,-43,-22,-70,-52,-59,-36,-58,-22,-32,-25,27,57,59,14,33,38,45,15,66,67,33,6,-41,-44,-49,-21,-45,-3,-38,
        -10,-64,-33,17,-37,-13,-33,-36,-29,-41,-17,-64,-6,30,49,22,10,25,22,24,-5,72,72,12,-21,-28,-56,-31,-17,4,14,-41,
        -34,-49,-33,-20,-46,-57,-64,-45,-57,10,-66,-64,-17,25,41,39,35,35,22,26,8,80,103,-43,-15,-40,-33,-54,-65,-67,-42,-45,
        -39,-31,-59,-48,-60,-45,-39,-38,5,-41,-67,-43,-44,-27,28,50,56,41,11,11,21,61,79,-38,-56,-49,-50,-45,-44,-49,-63,-64,
        -43,-57,-73,-42,-58,-61,-45,-24,-35,-41,15,-66,-51,-41,2,22,53,11,-13,2,-6,60,56,-26,-36,-68,-67,-65,-42,-51,-63,-59,
        -40,-44,-64,-56,-67,-64,53,-49,-42,-23,-57,-44,-37,-25,-12,-6,21,-11,-31,-2,45,83,-49,-62,-54,-61,-52,-64,-60,-6,-17,-17,
        -55,-55,-62,-68,-67,-62,17,-64,-60,-22,-65,3,-43,10,-12,20,-27,14,-5,10,66,89,48,83,48,69,57,76,72,70,77,87,
        -61,-64,-56,-58,-60,44,36,45,48,21,14,-6,16,23,2,15,-27,-14,0,16,101,61,56,64,67,80,68,76,58,51,48,47,
        -41,2,21,37,40,7,26,31,17,9,17,34,53,12,-10,13,-25,-18,-6,26,86,103,12,11,58,49,20,56,62,80,81,76,
        -33,30,40,45,43,24,15,56,42,33,2,4,6,47,76,53,-53,18,72,116,50,64,56,29,66,7,27,28,87,64,87,83,
        -68,41,42,29,27,49,23,25,39,25,6,-10,8,13,62,76,-51,13,85,101,34,74,61,48,42,55,81,60,40,48,62,80,
        -65,33,31,37,45,39,39,29,45,52,44,20,37,49,44,13,33,22,25,79,87,80,64,65,56,54,63,58,54,37,45,67,

        -48,-33,-27,-39,-37,-36,-55,-46,-14,-49,-24,-32,-37,-44,-39,-36,-38,-24,-32,-11,70,-16,-55,-41,-39,9,-55,-51,17,-32,-28,-12,
        -43,-33,-35,-34,-20,-43,20,-25,-34,3,-15,-21,32,-50,-38,-29,-29,-39,-1,8,-21,30,-39,-43,-30,9,-51,-50,34,-13,-39,8,
        -47,-41,-27,-37,-40,-31,-18,51,-33,3,4,-2,19,92,-12,-31,-32,-38,-16,-1,-43,102,19,4,-7,3,-28,-20,7,-14,-27,38,
        -57,-57,-12,-10,3,-41,-39,30,9,0,-16,24,21,28,7,-20,21,-9,-30,5,63,50,27,-43,38,6,3,10,-34,14,39,40,
        -56,-59,-36,-19,22,-25,-24,-16,-25,-39,-25,37,9,29,29,-34,-13,27,42,11,34,20,7,-63,22,20,56,34,-14,-30,10,0,
        -55,-58,-47,-39,12,7,-35,-26,-43,-43,-13,-34,3,15,53,7,14,-16,-44,34,49,36,8,-27,21,34,51,58,15,-26,19,-27,
        -55,-58,-51,-48,3,-2,-16,-25,-28,-20,-11,36,-16,-3,31,38,47,50,54,38,44,48,32,-16,7,19,-11,58,-3,0,11,-3,
        -52,-61,-59,-51,-59,-24,-23,-4,-35,-51,-20,54,-25,11,11,30,48,23,23,27,46,92,7,-32,0,8,-5,4,1,-5,2,11,
        -53,-59,-60,-58,-67,-55,-24,-2,-51,-43,-1,15,11,0,32,31,31,40,22,19,27,13,-11,-6,-1,6,-4,-28,-5,16,11,9,
        -54,-55,-43,-56,-47,-39,-24,-6,-61,-52,-61,28,12,25,-9,33,55,57,21,37,11,-5,-9,7,-5,-9,-14,2,22,34,3,22,
        -50,-51,0,-28,-12,-16,-8,34,2,12,-1,8,-9,-12,87,45,89,-2,45,39,50,-20,-7,23,0,2,0,7,11,16,-12,21,
        11,-47,3,-20,17,-1,-7,15,6,17,22,9,7,33,42,44,54,-73,68,30,84,19,27,50,47,37,11,34,29,39,30,23,
        -1,3,13,-2,23,27,12,10,16,14,31,15,-31,-31,37,49,18,64,17,19,76,46,65,38,57,37,8,24,27,36,50,26,
        14,19,12,12,23,11,16,37,21,27,16,29,2,14,41,-39,60,56,14,35,68,50,43,42,21,56,23,18,21,42,42,31,
        13,24,2,33,17,9,-1,-1,14,15,20,19,2,41,16,25,54,92,50,81,46,59,43,16,21,35,26,27,30,15,40,27,
        11,25,9,5,-12,11,8,4,16,23,22,27,13,25,16,37,95,104,90,79,39,58,53,29,5,10,14,19,16,15,12,5,
        19,13,19,9,4,1,22,-5,46,14,15,20,-1,10,36,38,70,87,69,54,38,57,61,-1,12,1,2,19,16,7,5,27,
        0,-1,-6,12,18,-20,0,4,31,14,23,15,23,21,60,74,92,47,57,58,50,59,67,-2,90,-1,4,4,7,-1,17,11,
        2,-19,10,15,38,-4,-1,9,15,19,21,16,18,55,67,72,65,51,39,54,54,54,78,30,-1,36,4,17,-9,27,6,18,
        -10,-6,12,11,9,30,27,19,-8,2,20,37,11,24,66,65,56,43,58,61,29,80,88,18,-16,9,15,15,11,10,-4,-11,
        26,-4,0,10,17,-24,3,-1,12,11,8,19,-10,30,58,63,30,53,58,65,41,89,89,51,45,15,3,11,27,17,23,-1,
        19,-30,4,33,7,7,3,-1,9,0,18,-5,11,41,58,37,30,46,46,45,19,88,92,44,7,4,1,18,24,37,35,5,
        -5,-6,3,12,6,-10,3,-5,-12,20,-42,3,11,33,51,54,52,46,40,46,31,92,109,0,20,-6,7,4,-26,-8,-8,4,
        2,0,-16,-25,-17,12,10,10,30,1,-9,-12,-4,-13,37,55,65,46,25,29,44,76,94,12,-17,-6,-8,4,-20,-20,-8,0,
        2,-24,-24,-5,-32,-32,-4,4,-1,15,36,-26,-10,-21,8,25,56,14,9,28,22,75,65,5,9,-42,-23,6,18,-22,-11,0,
        -11,-19,-29,-30,-20,-39,50,-16,2,10,-4,3,-2,-8,6,6,24,-3,-5,26,65,93,-24,-43,-9,-31,-16,-23,10,42,24,-8,
        -16,-35,-39,-44,-41,-46,21,-39,-21,10,7,17,-3,19,12,42,-22,19,14,34,81,100,52,85,54,74,61,80,75,73,79,90,
        -28,-29,-36,-39,-30,42,36,48,49,23,18,-3,20,23,19,35,-20,-4,19,41,103,58,58,66,69,81,72,81,66,58,55,54,
        -32,2,21,39,42,11,30,34,22,11,22,36,54,11,11,34,-27,-6,19,49,87,101,15,14,62,54,23,61,67,84,85,80,
        -10,31,43,48,47,29,23,61,47,38,8,7,7,49,81,55,-50,22,75,110,48,63,61,32,72,15,37,35,91,71,91,85,
        -46,44,46,34,30,54,30,32,43,30,10,-7,11,13,65,78,-48,15,88,102,38,77,66,51,46,58,85,63,42,52,65,81,
        -49,31,34,41,49,46,48,34,49,58,51,27,44,58,55,18,40,27,30,80,89,83,65,68,57,57,66,58,57,38,45,69,

        -76,-75,-74,-69,-67,-63,-81,-73,-39,-74,-60,-68,-74,-71,-66,-64,-65,-63,-72,-47,13,-60,-77,-63,-63,-34,-74,-76,-26,-62,-64,-45,
        -74,-78,-74,-63,-47,-73,-20,-60,-62,-28,-68,-62,6,-74,-67,-63,-60,-66,-60,-47,-61,-39,-68,-62,-57,-32,-73,-75,-15,-48,-70,-28,
        -74,-77,-59,-65,-65,-59,-55,5,-68,-47,-47,-46,46,96,-55,-65,-63,-68,-67,-57,-78,43,-34,-33,-47,-43,-61,-56,-42,-52,-60,3,
        -75,-78,-46,-43,-24,-63,-73,-20,-37,-51,-64,-24,50,54,-30,-60,-19,-52,-68,-48,17,13,-36,-68,-16,-40,-36,-28,-71,-20,-4,2,
        -74,-78,-67,-47,-8,-55,-55,-52,-66,-75,-64,-9,33,50,25,-69,-56,-13,-8,-31,60,45,-61,-79,-33,-30,0,-15,-51,-63,-29,-34,
        -72,-76,-77,-71,-23,-29,-67,-63,-70,-78,-63,-71,28,36,81,6,-16,-49,-75,63,77,63,-56,-60,-34,-10,0,1,-30,-55,-21,-55,
        -74,-76,-78,-79,-31,-31,-51,-62,-60,-68,-66,11,7,18,64,73,85,85,79,62,76,78,-25,-66,-51,-39,-59,3,-46,-39,-27,-41,
        -73,-79,-77,-77,-77,-53,-52,-35,-67,-78,-59,61,-1,34,35,60,81,60,57,55,77,104,-50,-74,-46,-45,-51,-40,-45,-47,-41,-28,
        -76,-78,-78,-79,-80,-71,-49,-28,-78,-73,-45,13,33,22,58,57,60,71,50,50,58,-14,-63,-42,-45,-41,-49,-64,-47,-24,-28,-30,
        -77,-78,-63,-76,-68,-62,-48,-32,-76,-71,-80,38,35,46,15,56,85,83,53,63,42,-51,-64,-26,-59,-51,-51,-33,-17,-5,-43,-25,
        -75,-76,-31,-53,-40,-43,-35,8,-27,-27,-41,0,11,-8,77,70,100,-1,71,68,74,-39,-55,-9,-51,-39,-34,-29,-17,-20,-44,-5,
        -12,-73,-29,-49,-8,-31,-39,-4,-23,-16,-8,4,25,60,61,71,77,-77,95,62,104,7,-11,11,3,-1,-24,1,-6,1,-5,-8,
        -30,-22,-14,-34,-6,0,-23,-18,-17,-19,-3,12,-11,-9,60,73,50,94,54,49,102,69,31,3,19,8,-20,-8,-1,5,16,-8,
        -11,-12,-13,-14,-5,-19,-12,7,-11,-6,-18,12,11,15,42,-8,72,80,46,65,96,89,30,15,-12,19,-9,-16,-5,11,5,-1,
        -12,1,-27,3,-13,-22,-31,-33,-12,-16,-13,-2,5,40,17,34,58,98,64,100,83,96,62,-8,-15,7,0,-5,-1,-12,3,-7,
        -12,2,-20,-16,-43,-18,-19,-17,-10,-12,-5,14,19,26,16,45,97,102,99,101,77,94,88,4,-27,-24,-15,-13,-10,-12,-20,-24,
        -1,-9,-9,-24,-25,-30,-5,-27,26,-20,-16,19,10,11,37,40,73,96,89,82,74,89,97,-23,-12,-21,-25,-11,-16,-20,-22,-5,
        -29,-30,-29,-14,-9,-32,-31,-27,11,-20,-4,20,33,22,61,75,96,72,78,88,85,94,100,-26,77,-28,-24,-25,-22,-32,-14,-19,
        -19,-44,-22,-3,16,-29,-25,-18,-9,-5,1,21,28,57,69,73,73,73,66,85,89,92,103,7,-28,14,-25,2,-31,7,-25,-16,
        -24,-25,-19,-12,-18,13,3,-4,-38,-22,12,23,18,29,68,66,68,65,81,89,61,105,104,8,-40,-19,-3,-12,-16,-16,-28,-33,
        11,-19,-24,-12,-5,-53,-24,-31,-12,-18,1,-10,-1,38,60,68,50,75,81,88,70,104,104,46,21,-12,-23,-20,4,-15,3,-24,
        -1,-51,-7,16,-18,-10,-18,-24,-12,-24,-2,-28,15,49,65,50,47,66,69,69,52,104,104,29,-8,-16,-25,0,-1,18,22,-16,
        -23,-34,-17,-8,-17,-32,-28,-29,-36,7,-59,-21,3,38,60,65,65,56,61,68,59,104,101,-21,-2,-24,-14,-20,-47,-32,-31,-25,
        -19,-22,-43,-44,-41,-12,-12,-16,11,-20,-34,-28,-24,-4,42,61,70,53,43,55,72,95,100,-9,-39,-26,-29,-20,-32,-38,-30,-26,
        -12,-43,-50,-24,-51,-51,-13,-16,-21,-10,14,-47,-28,-7,12,30,61,19,26,56,55,98,71,-8,-16,-57,-47,-20,-8,-36,-35,-27,
        -29,-36,-50,-46,-44,-57,42,-12,-12,-9,-31,-20,-12,5,24,18,30,4,17,56,87,103,-36,-58,-32,-40,-32,-43,-13,26,6,-10,
        -37,-51,-54,-62,-59,-64,11,-42,-32,-1,-13,4,-16,30,33,61,-15,26,35,62,104,105,57,91,58,80,66,85,80,76,83,92,
        -36,-46,-39,-44,-49,45,42,54,54,27,20,2,21,26,38,57,-13,4,42,69,104,65,64,71,74,89,76,87,69,62,59,58,
        -27,2,27,42,43,13,35,38,26,17,27,44,55,14,30,59,-20,8,43,75,96,101,21,23,68,60,27,66,73,86,90,84,
        -24,33,46,50,48,36,27,67,53,44,10,11,9,51,85,65,-46,27,81,104,54,69,66,38,76,19,42,43,96,75,93,90,
        -55,48,49,39,36,60,35,37,49,35,15,-5,14,15,73,83,-43,19,92,104,42,82,72,56,51,62,90,70,49,57,70,85,
        -59,40,39,46,53,50,53,40,51,61,55,30,48,63,61,23,44,27,32,85,97,89,73,73,63,65,72,65,62,42,50,74
    };
    //    int8_t weight_data[2][9] = {{1, 2, 3,
    //                                 0, 0, 0,
    //                                 -1, 1, -1},
    //                                {1, 1, 1,
    //                                 0, 0, 0,
    //                                 1, 1, 1}};

    //    int32_t bias_data[2] = {2, 5};
    //        int32_t bias_data[7] = { 74,
    //        	    -4008,
    //        	    -1859,
    //        	    760,    -1019,
    //        	    -3753,1};

    int32_t bias_data[6] = {
        74,
        -4008,
        -1859,
        760,
        -1019,
        -3753};

    //    int8_t wb_data[] = {
    //        1, 2, 3,
    //        0, 0, 0,
    //        -1, 1, -1,
    //        1, 1, 1,
    //        0, 0, 0,
    //        1, 1, 1,
    //        1, 2, 3,
    //        0, 0, 0,
    //        -1, 1, -1,
    //        1, 1, 1,
    //        0, 0, 0,
    //        1, 1, 1,
    //        };
    int8_t wb_data2[525]={
        14,-10,24,67,65,
        44,-49,-122,-40,127,
        40,69,-41,-111,14,
        -46,111,105,-45,-87,
        -112,-30,52,29,-27,

        -21,-44,-10,27,3,
        50,-46,-123,-47,100,
        55,76,-38,-110,1,
        -22,117,106,-47,-97,
        -53,4,75,37,-39,

        -42,-83,-40,5,-39,
        58,-52,-116,-34,85,
        66,73,-27,-97,-20,
        1,120,115,-37,-119,
        8,37,107,70,-29,

        26,21,26,38,88,
        1,5,22,27,45,
        -24,-35,-12,11,38,
        -73,-108,-92,-50,15,
        -92,-127,-108,-58,23,

        -17,-31,-39,-43,-21,
        26,40,50,31,4,
        37,51,70,60,24,
        -11,-15,-5,-3,-10,
        -48,-62,-53,-46,-34,

        6,-36,-65,-83,-71,
        56,48,40,5,-38,
        59,59,62,35,-20,
        36,23,20,6,-21,
        81,61,58,54,47,

        -19,84,102,34,8,
        7,127,79,-18,-25,
        44,65,-22,-47,5,
        16,-28,-72,-32,10,
        -61,-62,-29,4,-32,

        -51,-22,-23,-53,-22,
        12,42,-30,-91,-43,
        77,3,-104,-89,16,
        65,-63,-114,-24,69,
        20,-45,1,91,106,

        -49,-11,0,-9,48,
        6,56,1,-43,22,
        61,14,-79,-59,52,
        48,-57,-107,-25,61,
        13,-37,-3,70,70,

        15,-11,-44,-58,-51,
        4,-44,-48,-30,-35,
        -19,-9,31,33,-8,
        -28,17,30,-12,-20,
        13,15,-48,-94,-24,

        22,29,27,32,43,
        14,1,33,73,77,
        -27,22,102,127,94,
        -57,28,79,61,61,
        -34,6,-19,-39,35,

        -14,-52,-77,-67,-19,
        20,-54,-61,-29,3,
        0,-22,8,13,-3,
        -1,13,12,-35,-34,
        69,45,-33,-89,-25,

        26,8,45,29,-95,
        -12,-63,28,66,-69,
        15,-103,-18,63,-36,
        57,-69,-34,29,-25,
        75,-7,14,54,28,

        -13,-17,53,70,-40,
        -23,-70,52,127,13,
        15,-113,-3,117,42,
        45,-99,-51,46,15,
        10,-92,-66,0,-9,

        -13,-23,33,47,-59,
        -17,-68,34,92,-29,
        24,-97,-4,90,-8,
        62,-64,-23,46,-23,
        46,-37,-12,26,-33,

        15,-11,-44,-58,-51,
        4,-44,-48,-30,-35,
        -19,-9,31,33,-8,
        -28,17,30,-12,-20,
        13,15,-48,-94,-24,

        22,29,27,32,43,
        14,1,33,73,77,
        -27,22,102,127,94,
        -57,28,79,61,61,
        -34,6,-19,-39,35,

        -14,-52,-77,-67,-19,
        20,-54,-61,-29,3,
        0,-22,8,13,-3,
        -1,13,12,-35,-34,
        69,45,-33,-89,-25,

        26,8,45,29,-95,
        -12,-63,28,66,-69,
        15,-103,-18,63,-36,
        57,-69,-34,29,-25,
        75,-7,14,54,28,

        -13,-17,53,70,-40,
        -23,-70,52,127,13,
        15,-113,-3,117,42,
        45,-99,-51,46,15,
        10,-92,-66,0,-9,

        -13,-23,33,47,-59,
        -17,-68,34,92,-29,
        24,-97,-4,90,-8,
        62,-64,-23,46,-23,
        46,-37,-12,26,-33,
        };
    int8_t wb_data[450] = {
        14,-10,24,67,65,
        44,-49,-122,-40,127,
        40,69,-41,-111,14,
        -46,111,105,-45,-87,
        -112,-30,52,29,-27,

        -21,-44,-10,27,3,
        50,-46,-123,-47,100,
        55,76,-38,-110,1,
        -22,117,106,-47,-97,
        -53,4,75,37,-39,

        -42,-83,-40,5,-39,
        58,-52,-116,-34,85,
        66,73,-27,-97,-20,
        1,120,115,-37,-119,
        8,37,107,70,-29,

        26,21,26,38,88,
        1,5,22,27,45,
        -24,-35,-12,11,38,
        -73,-108,-92,-50,15,
        -92,-127,-108,-58,23,

        -17,-31,-39,-43,-21,
        26,40,50,31,4,
        37,51,70,60,24,
        -11,-15,-5,-3,-10,
        -48,-62,-53,-46,-34,

        6,-36,-65,-83,-71,
        56,48,40,5,-38,
        59,59,62,35,-20,
        36,23,20,6,-21,
        81,61,58,54,47,

        -19,84,102,34,8,
        7,127,79,-18,-25,
        44,65,-22,-47,5,
        16,-28,-72,-32,10,
        -61,-62,-29,4,-32,

        -51,-22,-23,-53,-22,
        12,42,-30,-91,-43,
        77,3,-104,-89,16,
        65,-63,-114,-24,69,
        20,-45,1,91,106,

        -49,-11,0,-9,48,
        6,56,1,-43,22,
        61,14,-79,-59,52,
        48,-57,-107,-25,61,
        13,-37,-3,70,70,

        15,-11,-44,-58,-51,
        4,-44,-48,-30,-35,
        -19,-9,31,33,-8,
        -28,17,30,-12,-20,
        13,15,-48,-94,-24,

        22,29,27,32,43,
        14,1,33,73,77,
        -27,22,102,127,94,
        -57,28,79,61,61,
        -34,6,-19,-39,35,

        -14,-52,-77,-67,-19,
        20,-54,-61,-29,3,
        0,-22,8,13,-3,
        -1,13,12,-35,-34,
        69,45,-33,-89,-25,

        26,8,45,29,-95,
        -12,-63,28,66,-69,
        15,-103,-18,63,-36,
        57,-69,-34,29,-25,
        75,-7,14,54,28,

        -13,-17,53,70,-40,
        -23,-70,52,127,13,
        15,-113,-3,117,42,
        45,-99,-51,46,15,
        10,-92,-66,0,-9,

        -13,-23,33,47,-59,
        -17,-68,34,92,-29,
        24,-97,-4,90,-8,
        62,-64,-23,46,-23,
        46,-37,-12,26,-33,

        -54,-41,-45,-40,-91,-40,-62,-63,-42,-57,-4,14,6,-23,-62,49,86,63,10,-46,110,127,106,74,30,30,12,12,44,24,-7,-38,-22,27,31,-29,-2,19,22,-4,-52,4,20,2,-44,-69,-32,-15,-10,-42,-56,-34,-4,49,41,-30,-32,3,61,67,-12,33,62,66,38,-25,38,52,34,-8,-62,-23,-11,-5,-30,
        };
    /* int8 data */
    /* int8 = clip(round(float32 / scale), -127, 127) */
    //    int8_t input_i8_data[16] = {25, -51, 25, 76,
    //                                -51, 76, 76, 127,
    //                                25, 0, -127, 127,
    //                                0, 0, 0, -25};
    //
    //    int8_t weight_i8_data[2][9] = {{42, 87, 127,
    //                                    0, 0, 0,
    //                                    -42, 42, -42},
    //                                   {127, 127, 127,
    //                                    0, 0, 0,
    //                                    127, 127, 127}};

    /* bias i32 = round(bias_fp32 / (input_scale * weight_scale)) */
    //    int bias_i32_data[2] = {537, 323};
    //
    //    int8_t bias_i8_data[2] = {25, 127};

    int n = 1;
    int c = 3;
    int h = 32;
    int w = 32;
    const char* test_node_name = "conv";
    int data_type = TENGINE_DT_INT8;
    int layout = TENGINE_LAYOUT_NCHW;

    // init
    int ret = test_graph_init();
    if (0 != ret)
        fprintf(stderr, "Tengine init failed.\n");

    // create
    graph_t graph = create_hcw_test_graph(test_node_name, data_type, layout, n, c, h, w, &create_test_conv_node,4);
    if (NULL == graph)
        return -1;

    //    set_log_level(LOG_INFO);
    //    dump_graph(graph);

    /* fill test data */
    // set quantize params
    struct tensor* input_tensor = (struct tensor*)get_graph_tensor(graph, "input_node");
    struct tensor* weight_tensor = (struct tensor*)get_graph_tensor(graph, "weight");
    struct tensor* bias_tensor = (struct tensor*)get_graph_tensor(graph, "bias");
    struct tensor* output_tensor = (struct tensor*)get_graph_tensor(graph, "conv");

    set_tensor_quant_param(input_tensor, &input_scale, &input_zero_point, 1);
    set_tensor_quant_param(weight_tensor, weight_scales, weight_zero_point, 2);
    set_tensor_quant_param(bias_tensor, bias_scales, bias_zero_point, 2);
    set_tensor_quant_param(output_tensor, &output_scale, &output_zero_point, 1);

    // set input data
    set_tensor_buffer(input_tensor, input_data, 3072 * sizeof(int8_t));

    // set weight data
    set_tensor_buffer(weight_tensor, wb_data, 450 * sizeof(int8_t));

    // set bias data
    set_tensor_buffer(bias_tensor, bias_data,6 * sizeof(int32_t));

    // graph run
    ret = test_graph_run(graph);
    if (0 != ret)
    {
        fprintf(stderr, "Run graph error. ERRNO: %d.\n", ret);
        test_graph_release(graph);
        return -1;
    }

    /* get output and dequant int8 to fp32 */
    int output_size = output_tensor->elem_num;
    int8_t* output_int8 = (int8_t*)output_tensor->data;

    //    std::vector<float> output_fp32(output_size);
    float* output_fp32 = (float*)malloc(output_size * sizeof(float));


    for (int i = 0; i < output_size; i++)
        output_fp32[i] = (float)output_int8[i];
    //    output_fp32[i] = (float)output_int8[i] * output_scale;

    /* check the result */
    ret = float_mismatch(output_fp32, reference_out, output_size);

    if (ret == 0)
        fprintf(stderr, "test pass.\n");
    else
        fprintf(stderr, "test failed.\n");

    // exit
    test_graph_release(graph);

    return ret;
    //    return 0;
}
